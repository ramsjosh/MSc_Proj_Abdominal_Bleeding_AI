{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMIp7UylIqX/AIMOMMV1OA/"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"egNJodFJae9c"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"markdown","source":["# **Package installation and imports**"],"metadata":{"id":"eTONcTD5aoew"}},{"cell_type":"code","source":["!pip install pandas numpy pydicom dicom2nifti tqdm matplotlib scikit-learn torch torchvision torchcam nibabel albumentations --quiet"],"metadata":{"id":"TWAPeGFQanBR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Restart session if got an error here**"],"metadata":{"id":"jB3nMP7ra90Y"}},{"cell_type":"code","source":["# Data handling and utilities\n","import pandas as pd\n","import numpy as np\n","from tqdm import tqdm\n","import glob\n","import tempfile\n","import heapq\n","import os\n","import pickle\n","import ast\n","import random\n","\n","# Plotting and visualization\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import matplotlib.patches as mpatches\n","import matplotlib.colors as mcolors\n","from scipy.ndimage import map_coordinates\n","\n","# PyTorch and model tools\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.utils.data import Dataset, DataLoader\n","from torch.nn.utils.rnn import pad_sequence\n","\n","# Vision and image processing\n","import pydicom                             # For reading DICOM CT slices\n","import torchvision.models as models        # Pretrained CNN models\n","import albumentations as A                 # Image augmentation\n","import cv2                                 # Image resizing and processing\n","import dicom2nifti                        # DICOM to NIfTI conversion\n","\n","# Explainable AI (Grad-CAM)\n","from torchcam.methods import GradCAM\n","\n","# Medical imaging tools\n","import nibabel as nib                      # For reading .nii segmentation masks\n","\n","# Evaluation and metrics\n","from sklearn.model_selection import GroupShuffleSplit\n","from sklearn.metrics import (\n","    roc_auc_score, confusion_matrix, accuracy_score,\n","    roc_curve, auc, precision_recall_curve, average_precision_score\n",")\n","\n","# Suppress unnecessary warnings\n","import warnings\n","warnings.filterwarnings(\"ignore\")"],"metadata":{"id":"VgHBKqJIa8mw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Copy and Extract Segmentation Masks to Colab Local Storage**"],"metadata":{"id":"JCgZicRYbHNg"}},{"cell_type":"code","source":["# Copy the ZIP from Drive to Colab local storage\n","!cp \"/content/drive/MyDrive/MSc_project/segmentations.zip\" \"/content/segmentations.zip\"\n","\n","# Unzip in Colab local storage\n","!unzip -q \"/content/segmentations.zip\" -d \"/content/segmentations/\"\n","\n","# For checking: confirm a few files exist\n","print(os.listdir('/content/segmentations/')[:5])"],"metadata":{"id":"TxKMPuADbH_4"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**DICOM Indexing and Label Matching**\n","\n","This section indexes all DICOM slices from the filtered patient/series folders and matches each slice to an \"Active Extravasation\" (bleeding) label using the provided CSV.\n","It builds a Python list of dicts with each DICOM's patient, series, slice index, label, and path."],"metadata":{"id":"-A--6e6ubLz3"}},{"cell_type":"markdown","source":["**RUN THIS ONLY ONCE**"],"metadata":{"id":"uxWHEz0UbPEI"}},{"cell_type":"code","source":["# import re\n","\n","# Paths\n","data_dir = '/content/drive/MyDrive/MSc_project/bleed_subset_images'\n","labels_csv = '/content/drive/MyDrive/MSc_project/image_level_labels_2024.csv'\n","labels_df = pd.read_csv(labels_csv)\n","\n","# Build the label dictionary for extravasation (bleeding only)\n","label_dict = {}\n","for _, row in labels_df.iterrows():\n","    label = 1 if row['injury_name'] == 'Active_Extravasation' else 0\n","    key = (str(row['patient_id']), str(row['series_id']), int(row['instance_number']))\n","    if label == 1:\n","        label_dict[key] = label  # Only mark extravasation-positive slices\n","\n","def extract_int(filename):\n","    \"\"\"\n","    Extract the integer slice number from DICOM filename.\n","    Handles formats like '796 (1)', '0050', '420', etc.\n","    \"\"\"\n","    num = re.findall(r'\\d+', filename)\n","    return int(num[0]) if num else None\n","\n","# Index all available DICOM files and match to labels\n","available = []\n","for patient_id in tqdm(os.listdir(data_dir), desc=\"Patients\"):\n","    patient_path = os.path.join(data_dir, patient_id)\n","    for series_id in os.listdir(patient_path):\n","        series_path = os.path.join(patient_path, series_id)\n","        for dcm_file in os.listdir(series_path):\n","            fname_no_ext = os.path.splitext(dcm_file)[0]\n","            slice_id = extract_int(fname_no_ext)\n","            if slice_id is None:\n","                print(f\"Warning: Could not extract integer from {dcm_file}\")\n","                continue\n","            key = (patient_id, series_id, slice_id)\n","            dcm_path = os.path.join(series_path, dcm_file)\n","            label = label_dict.get(key, 0)  # 1 for extravasation, else 0\n","            available.append({\n","                'patient_id': patient_id,\n","                'series_id': series_id,\n","                'slice_id': slice_id,\n","                'label': label,\n","                'dcm_path': dcm_path\n","            })\n","\n","print(f\"Total DICOMs indexed: {len(available)}\")"],"metadata":{"id":"dKUbvDG5bNPn"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Slice Selection for Model Input**\n","\n","This block selects which DICOM slices to process:\n","- **For positive (bleeding) cases:**  \n","  All labeled slices **plus their neighbors** (within `NEIGHBOR_RANGE` slices) are included for better context.\n","- **For negative cases:**  \n","  A fixed number of evenly spaced slices (`NEG_SLICES_PER_SERIES`) per series are sampled to balance the dataset.\n","- **Duplicates are removed** to avoid repeated slices.\n","\n","This is for balancing positive/negative data and ensuring enough context for 2.5D/stack-based modeling"],"metadata":{"id":"cozTed9xbkRP"}},{"cell_type":"markdown","source":["**RUN THIS ONLY ONCE**"],"metadata":{"id":"O8-lqH__bmf5"}},{"cell_type":"code","source":["# from collections import defaultdict\n","\n","NEIGHBOR_RANGE = 3             # Number of neighbor slices to include on each side of a positive\n","NEG_SLICES_PER_SERIES = 20     # Number of negative slices to sample per negative series\n","\n","# Organize slices by (patient, series)\n","slices_by_patient_series = defaultdict(list)\n","for item in available:\n","    key = (item['patient_id'], item['series_id'])\n","    slices_by_patient_series[key].append(item)\n","\n","# Select slices: all positive + neighbors, or downsample negatives\n","selected_records = []\n","\n","for (patient_id, series_id), slices in slices_by_patient_series.items():\n","    # Sort slices by their slice index\n","    slices = sorted(slices, key=lambda x: x['slice_id'])\n","    labels = [s['label'] for s in slices]\n","    if any(l == 1 for l in labels):  # Positive series\n","        # Add all positive slices plus NEIGHBOR_RANGE neighbors on each side\n","        for idx, l in enumerate(labels):\n","            if l == 1:\n","                for offset in range(-NEIGHBOR_RANGE, NEIGHBOR_RANGE+1):\n","                    nb_idx = idx + offset\n","                    if 0 <= nb_idx < len(slices):\n","                        selected_records.append(slices[nb_idx])\n","    else:  # Negative series\n","        # Sample evenly spaced negatives\n","        if len(slices) <= NEG_SLICES_PER_SERIES:\n","            selected_records.extend(slices)\n","        else:\n","            indices = np.linspace(0, len(slices)-1, NEG_SLICES_PER_SERIES, dtype=int)\n","            for idx in indices:\n","                selected_records.append(slices[idx])\n","\n","# Remove duplicate (patient, series, slice)\n","unique_keys = set()\n","final_records = []\n","for rec in selected_records:\n","    key = (rec['patient_id'], rec['series_id'], rec['slice_id'])\n","    if key not in unique_keys:\n","        final_records.append(rec)\n","        unique_keys.add(key)\n","\n","print(f\"Selected slices for processing: {len(final_records)}\")"],"metadata":{"id":"wZXtNYcbbk0D"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Save Slice/Label Index for Fast Reload**\n","\n","This block saves the **slice selection list** (`available` or `final_records`) and the optional `label_dict` as `.pkl` files for easy, fast access in future runs and no need to re-index DICOM or regenerate labels each time.\n"],"metadata":{"id":"Xkk8FD0qb5Hn"}},{"cell_type":"markdown","source":["**UNCOMMENT WHEN RUNNING**"],"metadata":{"id":"vSS57AxTb-j3"}},{"cell_type":"code","source":["# import pickle\n","\n","# Save the slice index\n","# with open('/content/drive/MyDrive/MSc_project/available_slices.pkl', 'wb') as f:\n","#     pickle.dump(available, f)   # Or: pickle.dump(final_records, f)\n","\n","# Save the label dictionary\n","# with open('/content/drive/MyDrive/MSc_project/label_dict.pkl', 'wb') as f:\n","#    pickle.dump(label_dict, f)"],"metadata":{"id":"ZjXUPCtFb50h"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Load Precomputed Slice and Label Index**\n","\n","This block **loads your precomputed list of selected DICOM slices** and the `label_dict` from pickle files.  \n","It is done to avoid reprocessing when resuming work to simply reload and continue.\n"],"metadata":{"id":"ZsXUL-5icPRH"}},{"cell_type":"code","source":["# import pickle\n","\n","with open('/content/drive/MyDrive/MSc_project/available_slices.pkl', 'rb') as f:\n","    available = pickle.load(f)\n","\n","with open('/content/drive/MyDrive/MSc_project/label_dict.pkl', 'rb') as f:\n","    label_dict = pickle.load(f)"],"metadata":{"id":"kdm-7IzccPpY"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**DICOM to 3-Channel NPY Conversion and Indexing**\n","\n","This block **preprocesses DICOM slices into NPY files** for fast model training:\n","- Loads each DICOM, applies HU scaling, resizes, and creates a 3-channel image using three standard CT windows (soft tissue, liver, blood).\n","- Saves each preprocessed array as `.npy` for efficient future access.\n","- Updates the dataframe with the generated NPY filenames for model indexing."],"metadata":{"id":"oPJhw9ICcS8e"}},{"cell_type":"code","source":["csv_path = '/content/drive/MyDrive/MSc_project/selected_slice_index.csv'\n","output_dir = '/content/drive/MyDrive/MSc_project/preproc_npy_3ch'\n","os.makedirs(output_dir, exist_ok=True)\n","\n","RESIZE_SHAPE = (256, 256)  # (width, height) for model input\n","\n","def apply_window(img, center, width):\n","    \"\"\"Apply CT windowing to HU values and scale to [0,1].\"\"\"\n","    lower = center - width // 2\n","    upper = center + width // 2\n","    img = np.clip(img, lower, upper)\n","    img = (img - lower) / (upper - lower)\n","    return img\n","\n","df = pd.read_csv(csv_path)\n","\n","for idx, row in tqdm(df.iterrows(), total=len(df)):\n","    dcm_path = row['dcm_path']\n","    label = row['label']\n","    patient_id = row['patient_id']\n","    series_id = row['series_id']\n","   slice_id = row['slice_id']\n","\n","    try:\n","       ds = pydicom.dcmread(dcm_path)\n","       arr = ds.pixel_array.astype(np.float32)\n","\n","        # Convert to Hounsfield Units (HU)\n","        if hasattr(ds, 'RescaleSlope') and hasattr(ds, 'RescaleIntercept'):\n","            arr = arr * float(ds.RescaleSlope) + float(ds.RescaleIntercept)\n","\n","        # Resize before windowing for efficiency\n","        if arr.shape != RESIZE_SHAPE[::-1]:\n","            arr = cv2.resize(arr, RESIZE_SHAPE, interpolation=cv2.INTER_LINEAR)\n","\n","        # Apply three windowings for multi-channel input\n","        arr_soft  = apply_window(arr, center=50, width=400)    # General abdomen\n","        arr_liver = apply_window(arr, center=60, width=150)    # Liver\n","        arr_blood = apply_window(arr, center=40, width=80)     # Blood/hemorrhage\n","\n","        arr_3ch = np.stack([arr_soft, arr_liver, arr_blood], axis=0)  # [3, H, W]\n","        arr_3ch = arr_3ch.astype(np.float32)\n","\n","        npy_name = f\"{patient_id}_{series_id}_{slice_id}_label{label}.npy\"\n","        npy_path = os.path.join(output_dir, npy_name)\n","        if os.path.exists(npy_path):\n","            continue\n","        np.save(npy_path, arr_3ch)\n","    except Exception as e:\n","        print(f\"Error processing {dcm_path}: {e}\")\n","\n","print(\"Preprocessing to 3-channel NPY complete.\")\n","\n","df['npy_file'] = df.apply(\n","    lambda r: f\"{r['patient_id']}_{r['series_id']}_{r['slice_id']}_label{r['label']}.npy\",\n","    axis=1\n",")\n","df.to_csv('/content/drive/MyDrive/MSc_project/preproc_npy_3ch_index.csv', index=False)"],"metadata":{"id":"M44JKtlAcThu"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**NPY File checking for 2.5D Input (3-Channel Per Slice)**\n","\n","**Just check for any corrupted files.**\n","\n","This block is **now only for quality check:** confirms all `.npy` have shape `[3, H, W]` and lists any corrupted ones."],"metadata":{"id":"BO1FftB1cp-l"}},{"cell_type":"markdown","source":["**RUN THIS ONLY ONCE FOR CHECKING**"],"metadata":{"id":"Xt4KR5qTc2JW"}},{"cell_type":"code","source":["npy_dir = '/content/drive/MyDrive/MSc_project/preproc_npy_3ch'\n","npy_files = sorted([f for f in os.listdir(npy_dir) if f.endswith('.npy')])\n","\n","records = []\n","for fname in tqdm(npy_files, desc=\"Checking NPY shapes\"):\n","    fpath = os.path.join(npy_dir, fname)\n","    arr = np.load(fpath, mmap_mode='r')  # Use mmap_mode to save memory\n","    records.append({\n","       'npy_file': fname,\n","        'shape_0': arr.shape[0],\n","        'shape_1': arr.shape[1],\n","        'shape_2': arr.shape[2]\n","    })\n","\n","shapes_df = pd.DataFrame(records)\n","shapes_df.to_csv('/content/drive/MyDrive/MSc_project/npy_shapes.csv', index=False)\n","\n","print(shapes_df['shape_0'].value_counts())\n","print(shapes_df.head())\n","\n","# Find any files that are not [3, H, W]\n","bad_files = shapes_df[shapes_df['shape_0'] != 3]\n","if not bad_files.empty:\n","    print(\"Problem files:\", bad_files)\n","else:\n","    print(\"All NPY files are valid [3, H, W].\")"],"metadata":{"id":"qY9d0tnWcqSv"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**2.5D Stack Construction: Sliding Window NPY Index**\n","\n","This **creates sliding stacks of NPY slices** for each patient/series, ready for 2.5D model training.  \n","- Each stack contains `stack_size` slices (`3` by default) centered on each slice.\n","- Edges are padded by repeating the nearest slice.\n","- Output is a DataFrame with patient/series, center slice, stack file list, and label (center slice label).\n","- Saved for efficient, balanced batch loading in PyTorch.\n"],"metadata":{"id":"QSh8y4wyc97u"}},{"cell_type":"code","source":["# Settings\n","csv_path = '/content/drive/MyDrive/MSc_project/preproc_npy_3ch_index.csv'\n","stack_size = 3  # Odd number, e.g. 3 for [prev, center, next]\n","\n","# Load NPY slice index\n","df = pd.read_csv(csv_path)\n","df['slice_id'] = df['slice_id'].astype(int)\n","\n","records = []\n","# Group by patient/series: build stacks (sliding window)\n","for (patient_id, series_id), group in df.groupby(['patient_id', 'series_id']):\n","    group = group.sort_values('slice_id').reset_index(drop=True)\n","    slice_ids = group['slice_id'].tolist()\n","    npy_files = group['npy_file'].tolist()\n","    labels = group['label'].tolist()\n","    n = len(slice_ids)\n","    for i in range(n):\n","        stack_idxs = []\n","        stack_npy_files = []\n","        # Symmetric context around center\n","        for offset in range(-(stack_size // 2), stack_size // 2 + 1):\n","            idx = i + offset\n","            idx = min(max(idx, 0), n - 1)  # Clamp to edge if out of bounds\n","            stack_idxs.append(slice_ids[idx])\n","            stack_npy_files.append(npy_files[idx])\n","        # Center slice's label for the stack\n","        records.append({\n","            'patient_id': patient_id,\n","            'series_id': series_id,\n","            'center_slice': slice_ids[i],\n","            'stack_slice_ids': stack_idxs,\n","            'stack_npy_files': stack_npy_files,\n","            'label': labels[i]\n","        })\n","\n","# Save DataFrame with stack info\n","stack_df = pd.DataFrame(records)\n","stack_df.to_csv('/content/drive/MyDrive/MSc_project/stack3_index.csv', index=False)\n","\n","print(stack_df.head())\n","print(f\"Total stacks created: {len(stack_df)}\")"],"metadata":{"id":"h-7Y-X61c-UV"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Match Segmentation Masks to Series**\n","\n","This checks **which patient/series in the stack index have available segmentation masks**.  \n","It adds a `has_mask` boolean column to the stack DataFrame for downstream use (segmentation/evaluation).\n"],"metadata":{"id":"rXdF8162dFF-"}},{"cell_type":"code","source":["# List available mask files\n","mask_series_ids = set(\n","    f.split('.')[0] for f in os.listdir('/content/segmentations/') if f.endswith('.nii')\n",")\n","\n","# Load your sliding window stack index\n","df = pd.read_csv('/content/drive/MyDrive/MSc_project/stack3_index.csv')\n","\n","# Check if this stack's series have a segmentation mask\n","df['has_mask'] = df['series_id'].astype(str).isin(mask_series_ids)\n","\n","# Summary of mask coverage\n","print(df['has_mask'].sum(), \"slices/stacks with masks found.\")\n","print(df[df['has_mask']].head())\n","print(\"Unique series_ids with masks in your data:\", df[df['has_mask']]['series_id'].unique())\n","\n","# Save the DataFrame with has_mask for future use\n","df.to_csv('/content/drive/MyDrive/MSc_project/stack3_index_with_mask.csv', index=False)"],"metadata":{"id":"mvLw9sQSdFZ1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Train/Validation/Test Split by Patient**\n","\n","This section splits the dataset into train, validation, and test sets at the patient level, making sure each patient appears in only one set. It uses `GroupShuffleSplit` to avoid patient overlap and ensure unbiased evaluation.\n"],"metadata":{"id":"iNDHlVx7dHqd"}},{"cell_type":"code","source":["# Copy master DataFrame for splitting\n","df_to_split = df.copy()\n","\n","# Assign patient-level label: 1 if any slice has bleeding, else 0\n","patient_labels = df_to_split.groupby('patient_id')['label'].max()\n","all_patient_ids = patient_labels.index\n","\n","# Split patients into training and temporary sets (val + test)\n","splitter = GroupShuffleSplit(test_size=0.3, n_splits=1, random_state=42)\n","train_inds, temp_inds = next(splitter.split(all_patient_ids, groups=all_patient_ids, y=patient_labels))\n","train_patient_ids = all_patient_ids[train_inds]\n","temp_patient_ids = all_patient_ids[temp_inds]\n","\n","# Further split temporary set equally into validation and test sets\n","val_test_splitter = GroupShuffleSplit(test_size=0.5, n_splits=1, random_state=42)\n","val_inds, test_inds = next(val_test_splitter.split(temp_patient_ids, groups=temp_patient_ids, y=patient_labels.loc[temp_patient_ids]))\n","val_patient_ids = temp_patient_ids[val_inds]\n","test_patient_ids = temp_patient_ids[test_inds]\n","\n","# Create DataFrames for each split\n","train_df = df_to_split[df_to_split['patient_id'].isin(train_patient_ids)]\n","val_df = df_to_split[df_to_split['patient_id'].isin(val_patient_ids)]\n","test_df = df_to_split[df_to_split['patient_id'].isin(test_patient_ids)]\n","\n","# Check that patients do not overlap between splits\n","assert len(set(train_df['patient_id']) & set(val_df['patient_id'])) == 0\n","assert len(set(train_df['patient_id']) & set(test_df['patient_id'])) == 0\n","assert len(set(val_df['patient_id']) & set(test_df['patient_id'])) == 0\n","\n","print(f\"Train Patients: {len(train_patient_ids)} | Validation Patients: {len(val_patient_ids)} | Test Patients: {len(test_patient_ids)}\")"],"metadata":{"id":"LQ-5sSf1dJWX"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Image Augmentation for CT Slices**\n","\n","This block defines a **data augmentation pipeline** using Albumentations, designed for 2.5D/3D medical images.\n","\n","[Albumentation Documentation](https://albumentations.ai/docs/3-basic-usage/keypoint-augmentations/)\n"],"metadata":{"id":"_uXsyWYJdLid"}},{"cell_type":"code","source":["# Compose image augmentations for training (applied per CT stack slice)\n","train_transform = A.Compose([\n","    A.HorizontalFlip(p=0.5), # Randomly flip images horizontally (50% chance)\n","    A.VerticalFlip(p=0.5), # Randomly flip images vertically (50% chance)\n","    A.RandomRotate90(p=0.5), # Randomly rotate by 90°, 180°, or 270° (50% chance)\n","    A.Affine(\n","        rotate=(-20, 20), # Random small rotation in range -20 to 20 degrees\n","        scale=(0.95, 1.05), # Random scale (zoom in/out by up to 5%)\n","        translate_percent={ # Random translation up to 5% of image size in x/y\n","            \"x\": (-0.05, 0.05),\n","            \"y\": (-0.05, 0.05)\n","        },\n","        p=0.5\n","    ),\n","    A.RandomBrightnessContrast(p=0.2),  # Randomly adjust brightness and contrast (20% chance)\n","    A.RandomGamma(gamma_limit=(85, 115), p=0.2),  # Small random gamma changes (20% chance)\n","    A.Blur(blur_limit=3, p=0.1),           # Slight blur (3x3 kernel, 10% chance)\n","    A.ElasticTransform(alpha=10, sigma=1, p=0.05),  # Elastic deformation for more realistic variation (5% chance)\n","])"],"metadata":{"id":"BTaibFIHdNRG"},"execution_count":null,"outputs":[]}]}